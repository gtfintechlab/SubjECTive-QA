import argparse
import re
import pandas as pd
import time
from datetime import date
from pathlib import Path
from openai import OpenAI
import logging
from datasets import load_dataset
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

client = OpenAI()

definition_map = {
    "RELEVANT": "The speaker has answered the question entirely and appropriately.",
    "SPECIFIC": "The speaker includes specific and technical details in the answer.",
    "CAUTIOUS": "The speaker answers using a more conservative, risk-averse approach.",
    "ASSERTIVE": "The speaker answers with certainty about the company's events.",
    "CLEAR": "The speaker is transparent in the answer and about the message to be conveyed.",
    "OPTIMISTIC": "The speaker answers with a positive tone regarding outcomes.",
}

def parse_arguments():
    """
    Parses command-line arguments for running a Large Language Model (LLM) using OpenAI's API.

    Returns:
        argparse.Namespace: A namespace object containing the following parsed arguments:
        
        - model (str): The name of the model to use (required).
        - feature (str): The feature to evaluate using the model (required).
        - api_key (str): The API key to authenticate the request (required).
        - max_tokens (int): The maximum number of tokens to generate. Default is 128.
        - temperature (float): The sampling temperature to use. Controls randomness. Default is 0.7.
        - top_p (float): Nucleus sampling top-p value. Default is 0.7.
        - frequency_penalty (float): The penalty for frequent token repetition. Default is 0.0.
        - seed (str): The random seed to use for reproducibility. Default is '5768'.
    
    Usage:
        This function is used to parse the arguments needed to run the OpenAI model. Example:

        ```
        python script.py --model "gpt-3.5-turbo" --feature "summarization" --api_key "your_api_key"
        ```

        Optional parameters like max_tokens, temperature, top_p, and others can be adjusted as needed.
    
    Returns:
        argparse.Namespace: Parsed arguments accessible as attributes, such as args.model, args.feature, etc.
    """
    parser = argparse.ArgumentParser(description="Run a LLM on OpenAI")
    parser.add_argument("--model", type=str, help="Model to use", required=True)
    parser.add_argument("--feature", type=str, help="Feature to evaluate", required=True)
    parser.add_argument("--api_key", type=str, help="API key to use", required=True)
    parser.add_argument("--max_tokens", type=int, default=128, help="Max tokens to use")
    parser.add_argument("--temperature", type=float, default=0.7, help="Temperature to use")
    parser.add_argument("--top_p", type=float, default=0.7, help="Top-p to use")
    parser.add_argument("--frequency_penalty", type=float, default=0.0, help="Frequency penalty to use")
    parser.add_argument("--seed", type=str, default='5768', help="Seed to use")
    return parser.parse_args()

def extract_label(text, label_regex):
    match = re.search(label_regex, text)
    return match.group(1) if match else "None"

def inference(args):
    """
    Runs inference using a specified language model (LLM) to classify a feature in a dataset and save results.

    Args:
        args (argparse.Namespace): Parsed arguments containing the following:
            - feature (str): The feature to evaluate from the dataset.
            - model (str): The name of the LLM model to use.
            - api_key (str): API key for authenticating the LLM request.
            - max_tokens (int): Maximum number of tokens to generate in the LLM response.
            - temperature (float): Temperature value for controlling the randomness of the LLM output.
            - top_p (float): Top-p sampling value for nucleus sampling.
            - frequency_penalty (float): Penalty for repeating tokens in LLM output.
            - seed (str): Seed value for loading the dataset with reproducibility.

    Returns:
        pandas.DataFrame: A DataFrame containing the following columns:
            - questions: List of questions processed from the dataset.
            - answers: Corresponding answers from the dataset.
            - llm_responses: Responses generated by the LLM model for the feature.
            - actual_labels: Actual labels for the feature from the dataset.
            - complete_responses: Full LLM response objects.

    The function processes each question in the dataset, sends it to the LLM model, and stores the responses
    and actual labels. Results are incrementally saved to a CSV file after each iteration to allow progress tracking
    and prevent data loss in case of errors.


    Notes:
        - Results are saved incrementally to a CSV file in the format: `results/{feature}/{model}/{feature}_{dd_mm_yyyy}.csv`.
        - The function includes delays between LLM requests to avoid hitting API rate limits.
        - Errors encountered during processing will be logged, and the function will retry with a longer delay.

    Raises:
        Exception: If an error occurs during the processing of a dataset row, it will be logged and the row will be retried after a delay.
    """
    feature = args.feature.strip('“”"')
    dataset = load_dataset("gtfitechlab/SubjECTive-QA", split="test", seed=args.seed)

    # Initialize lists to store actual labels and model responses
    questions = []
    answers = []
    llm_responses = []
    feature_labels = []
    complete_responses = []
    gpt_model = args.model.strip('“”"')

    # Path to save the results
    results_path = (
        Path.cwd()
        / "results"
        / feature
        / args.model
        / f"{feature}_{date.today().strftime('%d_%m_%Y')}.csv"
    )
    results_path.parent.mkdir(parents=True, exist_ok=True)

    # Iterating through the dataset
    start_t = time.time()
    df = None  # Initialize df here to avoid UnboundLocalError
    for i in range(len(dataset)):
        question = dataset.iloc[i]["QUESTION"]
        logger.info(f"Processing question: {question}")
        answer = dataset.iloc[i]["ANSWER"]
        actual_label = dataset.iloc[i][feature]
        questions.append(question)
        answers.append(answer)
        feature_labels.append(actual_label)
        try:
            response = client.chat.completions.create(
                model=gpt_model,
                messages=[
                    {"role": "system", "content": "You are an expert sentence classifier."},
                    {"role": "user", "content": gpt_prompt(feature, definition_map[feature], question, answer)},
                ],
                max_tokens=args.max_tokens,
                temperature=args.temperature,
                top_p=args.top_p,
                frequency_penalty=args.frequency_penalty
            )
            complete_responses.append(response)
            response_label = response.choices[0].message.content
            llm_responses.append(response_label)
            logger.info(f"Processed row {i + 1}/{len(dataset)}: {response_label}")

            # Create DataFrame after each row is processed
            df = pd.DataFrame(
                {
                    "questions": questions,
                    "answers": answers,
                    "llm_responses": llm_responses,
                    "actual_labels": feature_labels,
                    "complete_responses": complete_responses,
                }
            )

            # Save the DataFrame after each iteration
            df.to_csv(results_path, index=False)
            logger.info(f"Saved progress to {results_path}")

            # Delay between requests to avoid hitting rate limits
            time.sleep(7.0)
        except Exception as e:
            logger.error(f"Error processing row {i}: {e}")
            time.sleep(8.0)  # Additional delay on error

    if df is not None:
        logger.info(f"Final results saved to {results_path}")
    else:
        logger.warning("No data processed.")

    return df

def gpt_prompt(feature, definition, question, answer):
    user_msg = f"""Given the following feature: {feature} and its corresponding definition: {definition}\n
                   Give the answer a rating of:\n
                   2: If the answer positively demonstrates the chosen feature, with regards to the question.\n
                   1: If there is no evident/neutral correlation between the question and the answer for the feature.\n
                   0: If the answer negatively correlates to the question on the chosen feature.\n
                   Provide the rating only. No explanations. This is the question: {question} and this is the answer: {answer}."""
    return user_msg

def main():
    args = parse_arguments()
    feature = args.feature.strip('“”"')

    if feature in definition_map:
        start_t = time.time()
        df = inference(args)
        time_taken = time.time() - start_t
        logger.info(f"Processing time: {time_taken} seconds")
    else:
        logger.error(f"Feature '{args.feature}' not found in the feature definition map.")

if __name__ == "__main__":
    main()
